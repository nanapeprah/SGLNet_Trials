{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXRUOjyWL8HoERrpuUg7Dt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nanapeprah/SGLNet_Trials/blob/main/SGLNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ymt4LWBsW4ia"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "from torch_geometric.nn import EGConv\n",
        "\n",
        "# from data.structureGraph import *\n",
        "from model.surrogate_fun import ActFun, Erf, ATan, ATan_P, Erf_P\n",
        "from model.lstmcell import LSTMCell\n",
        "\n",
        "\n",
        "# surrogate_function.\n",
        "# act_enc_fun = ActFun.apply   # encode raw signal to spike train.\n",
        "# act_fun = ActFun.apply\n",
        "\n",
        "act_enc_fun = ATan.apply\n",
        "act_fun = ATan.apply\n",
        "erf_fun = Erf.apply\n",
        "erf_p = Erf_P.apply\n",
        "\n",
        "\n",
        "# membrane potential update, for GCN using edge weight\n",
        "def mem_update_conv_weight(ops, x, edge_idxs, edge_weight, mem, spike, thresh, lens, decay):\n",
        "    ops_ret = ops(x, edge_idxs, edge_weight).view(mem.shape)  # mini-batch graph conv,then reshape to each batch.\n",
        "    mem = mem * decay * (1. - spike) + ops_ret\n",
        "    spike = act_fun(mem, thresh, lens) # act_fun : approximation firing function\n",
        "    return mem, spike\n",
        "\n",
        "def mem_update_conv_weight2(ops, x, edge_idxs, edge_weight, mem, spike, thresh, alpha, decay):\n",
        "    ops_ret = ops(x, edge_idxs, edge_weight).view(mem.shape)  # mini-batch graph conv,then reshape to each batch.\n",
        "    mem = mem * decay * (1. - spike) + ops_ret\n",
        "    spike = act_fun(mem, thresh, alpha) # act_fun : approximation firing function\n",
        "    return mem, spike\n",
        "\n",
        "# membrane potential update, for GCN\n",
        "def mem_update_conv(ops, x, edge_idxs, mem, spike, thresh, lens, decay):\n",
        "    ops_ret = ops(x, edge_idxs).view(mem.shape)  # mini-batch graph conv,then reshape to each batch.\n",
        "    mem = mem * decay * (1. - spike) + ops_ret\n",
        "    spike = act_fun(mem, thresh, lens) # act_fun : approximation firing function\n",
        "    return mem, spike\n",
        "\n",
        "# membrane potential update, for GCN\n",
        "def mem_update_conv2(ops, x, edge_idxs, mem, spike, thresh, alpha, decay):\n",
        "    ops_ret = ops(x, edge_idxs).view(mem.shape)  # mini-batch graph conv,then reshape to each batch.\n",
        "    mem = mem * decay * (1. - spike) + ops_ret\n",
        "    spike = act_fun(mem, thresh, alpha) # act_fun : approximation firing function\n",
        "    return mem, spike\n",
        "\n",
        "def mem_update(ops, x, mem, spike, thresh, lens, decay):\n",
        "    mem = mem * decay * (1. - spike) + ops(x)\n",
        "    spike = act_fun(mem, thresh, lens)\n",
        "    return mem, spike\n",
        "\n",
        "def mem_update2(ops, x, mem, spike, thresh, alpha, decay):\n",
        "    mem = mem * decay * (1. - spike) + ops(x)\n",
        "    spike = act_fun(mem, thresh, alpha)\n",
        "    return mem, spike\n",
        "\n",
        "\n",
        "class GraLstmEnc(nn.Module):\n",
        "    def __init__(self, params, device=\"cuda:0\"):\n",
        "        super(GraLstmEnc, self).__init__()\n",
        "\n",
        "        # get params.\n",
        "        self.cfg_enc = np.array(str.split(params['cfg_enc'], ','), dtype=int)  # encoding scheme transform raw signals to spike trains.\n",
        "        self.cfg_gnn = np.array(str.split(params['cfg_gnn'], ','), dtype=int)  # gnn_layer(in_channels, out_channels)\n",
        "        self.cfg_s = np.array(str.split(params['cfg_s'], ','), dtype=int)  # node number.\n",
        "        self.cfg_fc = np.array(str.split(params['cfg_fc'], ','), dtype=int) # fully connect.\n",
        "        self.num_classes = int(params['num_classes']) # class number.\n",
        "        self.num_heads = int(params['num_heads']) # EGC num_heads.\n",
        "        self.num_bases = int(params['num_bases']) # EGC num_bases\n",
        "        self.gamma = float(params['gamma']) # dropout coefficient.\n",
        "        # neuronal threshold,  hyper-parameters of approximate function\n",
        "        self.thresh, self.lens, self.decay = float(params['thresh']), float(params['lens']), float(params['decay'])\n",
        "        # encoding step, act_enc_fun parameters.\n",
        "        self.enc_thresh, self.enc_lens = float(params['enc_thresh']), float(params['enc_lens'])\n",
        "\n",
        "        # define the module.\n",
        "        in_enc_planes, out_enc_planes = self.cfg_enc # encoding feature dim\n",
        "        in_planes, out_planes = self.cfg_gnn # graph conv feature dim.\n",
        "        self.encoding = nn.Linear(in_enc_planes, out_enc_planes)  # linear transformation to encode, behind with non-linear activation.\n",
        "        self.conv1 = EGConv(in_planes, out_planes, num_heads= self.num_heads, num_bases=self.num_bases)\n",
        "\n",
        "        self.lstm_cell1 = LSTMCell(input_size = self.cfg_s[-1] * out_planes, hidden_size = int(self.cfg_fc[0]), surrogate_fun1 = erf_p, alpha=2.0, surrogate_fun2=erf_fun)\n",
        "        self.fc1 = nn.Linear(self.cfg_fc[0], self.cfg_fc[1])\n",
        "        self.fc2 = nn.Linear(self.cfg_fc[1], self.num_classes)\n",
        "        self.device = device\n",
        "\n",
        "    def forward(self, input):\n",
        "        x_data, edge_data = input.x, input.edge_index   # min-batch method.\n",
        "        data = x_data.to(self.device)  #[batch_size, channel, fet_dim, time_win]\n",
        "        sizes = data.size()\n",
        "        time_window = sizes[-1]\n",
        "\n",
        "        # mini-batch sample number.\n",
        "        sample_num = int(sizes[0] / self.cfg_s[0])     # each samle is 64 channels, so all the channels divide 64 equal the number of sample.\n",
        "        enc_rest = []\n",
        "        output_spike = []; output_mem = []\n",
        "        enc_mem = enc_spike = enc_sumspike = torch.zeros(sample_num * self.cfg_s[0], self.cfg_enc[1], device = self.device) # enc.\n",
        "\n",
        "        c1_mem = c1_spike = torch.zeros(sample_num, self.cfg_s[0], self.cfg_gnn[1], device=self.device) # graph conv.\n",
        "        lh1_spike = torch.zeros(sample_num, self.cfg_fc[0], dtype = torch.float, device = self.device)  # lstm h,\n",
        "        lc1_spike = torch.zeros(sample_num, self.cfg_fc[0], dtype = torch.float, device = self.device)  # lstm c,\n",
        "\n",
        "        h1_mem = h1_spike = h1_sumspike  = torch.zeros(sample_num, self.cfg_fc[1], device=self.device)  # fc1\n",
        "        h2_mem = h2_spike = h2_sumspike  = torch.zeros(sample_num, self.num_classes, device=self.device)  # fc2\n",
        "\n",
        "        inputs = data.split(1, dim=len(sizes)-1)\n",
        "        for step in range(time_window):   # simulation time steps\n",
        "            # 1. prepare the data.\n",
        "            # x = inputs[step].squeeze().unsqueeze(dim=1)  # squeeze the last dimension. for 1 dimentional feature case.\n",
        "            x = inputs[step].squeeze()\n",
        "\n",
        "            # data, edge put to device.\n",
        "            x = x.to(self.device)\n",
        "            edge_idxs = edge_data.to(self.device)\n",
        "\n",
        "            # encode raw signal to spike trains.\n",
        "            # x = act_enc_fun(self.encoding(x), self.enc_thresh, self.enc_lens)  # only using act_fun\n",
        "            enc_mem, enc_spike = mem_update2(self.encoding, x, enc_mem, enc_spike, self.enc_thresh, self.enc_lens, self.decay)  # using lif neuro.\n",
        "            x = enc_spike\n",
        "            # enc_rest.append(x.cpu().numpy())\n",
        "            enc_sumspike += x\n",
        "\n",
        "            # 2. graph conv.\n",
        "            c1_mem, c1_spike = mem_update_conv2(self.conv1, x, edge_idxs, c1_mem, c1_spike, self.thresh, self.lens, self.decay)\n",
        "\n",
        "            # 3. lstm conv.\n",
        "            x = c1_spike\n",
        "            x = x.view(sample_num, -1)\n",
        "            lh1_spike, lc1_spike = self.lstm_cell1(x, hc = (lh1_spike, lc1_spike))\n",
        "\n",
        "            # 4. fc step.\n",
        "            h1_mem, h1_spike = mem_update2(self.fc1, lh1_spike, h1_mem, h1_spike, self.thresh, self.lens, self.decay)\n",
        "            h1_sumspike += h1_spike\n",
        "\n",
        "            h2_mem, h2_spike = mem_update2(self.fc2, h1_spike, h2_mem, h2_spike, self.thresh, self.lens, self.decay)\n",
        "            h2_sumspike += h2_spike\n",
        "\n",
        "            # record output spikes and mems.\n",
        "            # output_mem.append(h2_mem.cpu().numpy())\n",
        "            # output_spike.append(h2_spike.cpu().numpy())\n",
        "\n",
        "        outputs = h2_sumspike / time_window\n",
        "        # return outputs, enc_sumspike, h1_sumspike, enc_rest, output_mem, output_spike  # out, encoder_init_feats, hidden_feats, encoder_result.\n",
        "        return outputs  # only output the outputs."
      ]
    }
  ]
}